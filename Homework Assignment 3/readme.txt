1. part1&2.ipynb => unigram model based on uniform both probabilities and frequencies.
2. part3_reddit.ipynb => n-gram model for the reddit.txt on train file.
3. part3_ted_txt.ipynb => n-gram model for ted.txt on train file.
4. part3_perplexity_non_smoothing.ipynb => perplexity plots without smoothing.
5. part4_reddit.ipynb => n-gram model after implementing the Laplace smoothing on reddit.txt as the train file.
6. part4_ted.ipynb => n-gram model after implementing the Laplace smoothing on ted.txt as the train file.
7. part4_perplexity_smoothing.ipynb => perplexity plot without smoothing.
8. text_generation_reddit.ipynb => text generation code on reddit.out.
9. text_generation_ted_out.ipynb => text generation code on ted.out.



Perplexity Scores for both Part 1 and Part 2:
Perplexity Score of file :test.ted.txt => 53681
Perplexity Score of file :test.news.txt => 53681
Perplexity Score of file :test.reddit.txt => 53681

Note: As all the programs are implemented and run on the python-jupyter-notebook. All the results and outputs are saved on the output console of
the jupyter notebook. You can look them or run the program again to see the new output and results.